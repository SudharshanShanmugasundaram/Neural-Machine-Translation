{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n",
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[wow] => [potzdonner]\n",
      "[wow] => [donnerwetter]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stop] => [stopp]\n",
      "[wait] => [warte]\n",
      "[go on] => [mach weiter]\n",
      "[hello] => [hallo]\n",
      "[i ran] => [ich rannte]\n",
      "[i see] => [ich verstehe]\n",
      "[i see] => [aha]\n",
      "[i try] => [ich probiere es]\n",
      "[i won] => [ich hab gewonnen]\n",
      "[i won] => [ich habe gewonnen]\n",
      "[smile] => [lacheln]\n",
      "[cheers] => [zum wohl]\n",
      "[freeze] => [keine bewegung]\n",
      "[freeze] => [stehenbleiben]\n",
      "[got it] => [kapiert]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [einverstanden]\n",
      "[he ran] => [er rannte]\n",
      "[he ran] => [er lief]\n",
      "[hop in] => [mach mit]\n",
      "[hug me] => [druck mich]\n",
      "[hug me] => [nimm mich in den arm]\n",
      "[hug me] => [umarme mich]\n",
      "[i fell] => [ich fiel]\n",
      "[i fell] => [ich fiel hin]\n",
      "[i fell] => [ich sturzte]\n",
      "[i fell] => [ich bin hingefallen]\n",
      "[i fell] => [ich bin gesturzt]\n",
      "[i know] => [ich wei]\n",
      "[i lied] => [ich habe gelogen]\n",
      "[i lost] => [ich habe verloren]\n",
      "[i paid] => [ich habe bezahlt]\n",
      "[i paid] => [ich zahlte]\n",
      "[i swim] => [ich schwimme]\n",
      "[im] => [ich bin jahre alt]\n",
      "[im] => [ich bin]\n",
      "[im ok] => [mir gehts gut]\n",
      "[im ok] => [es geht mir gut]\n",
      "[im up] => [ich bin wach]\n",
      "[im up] => [ich bin auf]\n",
      "[no way] => [unmoglich]\n",
      "[no way] => [das kommt nicht in frage]\n",
      "[no way] => [das gibts doch nicht]\n",
      "[no way] => [ausgeschlossen]\n",
      "[no way] => [in keinster weise]\n",
      "[really] => [wirklich]\n",
      "[really] => [echt]\n",
      "[really] => [im ernst]\n",
      "[thanks] => [danke]\n",
      "[try it] => [versuchs]\n",
      "[why me] => [warum ich]\n",
      "[ask tom] => [frag tom]\n",
      "[ask tom] => [fragen sie tom]\n",
      "[ask tom] => [fragt tom]\n",
      "[be cool] => [entspann dich]\n",
      "[be fair] => [sei nicht ungerecht]\n",
      "[be fair] => [sei fair]\n",
      "[be nice] => [sei nett]\n",
      "[be nice] => [seien sie nett]\n",
      "[beat it] => [geh weg]\n",
      "[beat it] => [hau ab]\n",
      "[beat it] => [verschwinde]\n",
      "[beat it] => [verdufte]\n",
      "[beat it] => [mach dich fort]\n",
      "[beat it] => [zieh leine]\n",
      "[beat it] => [mach dich vom acker]\n",
      "[beat it] => [verzieh dich]\n",
      "[beat it] => [verkrumele dich]\n",
      "[beat it] => [troll dich]\n",
      "[beat it] => [zisch ab]\n",
      "[beat it] => [pack dich]\n",
      "[beat it] => [mach ne fliege]\n",
      "[beat it] => [schwirr ab]\n",
      "[beat it] => [mach die sause]\n",
      "[beat it] => [scher dich weg]\n",
      "[beat it] => [scher dich fort]\n",
      "[call me] => [ruf mich an]\n",
      "[come in] => [komm herein]\n",
      "[come in] => [herein]\n",
      "[come on] => [komm]\n",
      "[come on] => [kommt]\n",
      "[come on] => [mach schon]\n",
      "[come on] => [macht schon]\n",
      "[come on] => [komm schon]\n",
      "[get tom] => [hol tom]\n",
      "[get out] => [raus]\n",
      "[get out] => [geh raus]\n",
      "[get out] => [geht raus]\n",
      "[go away] => [geh weg]\n",
      "[go away] => [hau ab]\n",
      "[go away] => [verschwinde]\n",
      "[go away] => [verdufte]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    " \n",
    "def load_doc(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    " \n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            line = line.split()\n",
    "            line = [word.lower() for word in line]\n",
    "            line = [word.translate(table) for word in line]\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n",
    " \n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "filename = './data/deu.txt'\n",
    "doc = load_doc(filename)\n",
    "pairs = to_pairs(doc)\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')\n",
    "for i in range(100):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "shuffle(dataset)\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2309\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3657\n",
      "German Max Length: 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 256)           936192    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 2309)           593413    \n",
      "=================================================================\n",
      "Total params: 2,580,229\n",
      "Trainable params: 2,580,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/32\n",
      "9000/9000 [==============================] - 70s 8ms/step - loss: 4.3098 - val_loss: 3.5469\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.54686, saving model to model.h5\n",
      "Epoch 2/32\n",
      "9000/9000 [==============================] - 64s 7ms/step - loss: 3.4022 - val_loss: 3.3886\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.54686 to 3.38863, saving model to model.h5\n",
      "Epoch 3/32\n",
      "9000/9000 [==============================] - 64s 7ms/step - loss: 3.2469 - val_loss: 3.3115\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.38863 to 3.31150, saving model to model.h5\n",
      "Epoch 4/32\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 3.1011 - val_loss: 3.2076\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.31150 to 3.20757, saving model to model.h5\n",
      "Epoch 5/32\n",
      "9000/9000 [==============================] - 67s 7ms/step - loss: 2.9643 - val_loss: 3.1017\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.20757 to 3.10165, saving model to model.h5\n",
      "Epoch 6/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 2.7821 - val_loss: 2.9646\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.10165 to 2.96461, saving model to model.h5\n",
      "Epoch 7/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 2.5992 - val_loss: 2.8479\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.96461 to 2.84791, saving model to model.h5\n",
      "Epoch 8/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 2.4369 - val_loss: 2.7501\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.84791 to 2.75013, saving model to model.h5\n",
      "Epoch 9/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 2.2851 - val_loss: 2.6596\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.75013 to 2.65959, saving model to model.h5\n",
      "Epoch 10/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 2.1428 - val_loss: 2.5870\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.65959 to 2.58699, saving model to model.h5\n",
      "Epoch 11/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 2.0084 - val_loss: 2.5212\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.58699 to 2.52124, saving model to model.h5\n",
      "Epoch 12/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.8809 - val_loss: 2.4646\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.52124 to 2.46460, saving model to model.h5\n",
      "Epoch 13/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.7585 - val_loss: 2.4035\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.46460 to 2.40350, saving model to model.h5\n",
      "Epoch 14/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.6428 - val_loss: 2.3443\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.40350 to 2.34433, saving model to model.h5\n",
      "Epoch 15/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.5287 - val_loss: 2.3101\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.34433 to 2.31012, saving model to model.h5\n",
      "Epoch 16/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.4237 - val_loss: 2.2751\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.31012 to 2.27513, saving model to model.h5\n",
      "Epoch 17/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.3183 - val_loss: 2.2315\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.27513 to 2.23146, saving model to model.h5\n",
      "Epoch 18/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.2233 - val_loss: 2.2023\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.23146 to 2.20233, saving model to model.h5\n",
      "Epoch 19/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.1313 - val_loss: 2.1655\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.20233 to 2.16552, saving model to model.h5\n",
      "Epoch 20/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.0406 - val_loss: 2.1272\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.16552 to 2.12722, saving model to model.h5\n",
      "Epoch 21/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.9536 - val_loss: 2.1166\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.12722 to 2.11663, saving model to model.h5\n",
      "Epoch 22/32\n",
      "9000/9000 [==============================] - 64s 7ms/step - loss: 0.8752 - val_loss: 2.1078\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.11663 to 2.10775, saving model to model.h5\n",
      "Epoch 23/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.8025 - val_loss: 2.0798\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.10775 to 2.07976, saving model to model.h5\n",
      "Epoch 24/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.7347 - val_loss: 2.0709\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.07976 to 2.07093, saving model to model.h5\n",
      "Epoch 25/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.6687 - val_loss: 2.0544\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.07093 to 2.05437, saving model to model.h5\n",
      "Epoch 26/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.6095 - val_loss: 2.0439\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.05437 to 2.04389, saving model to model.h5\n",
      "Epoch 27/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.5598 - val_loss: 2.0373\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.04389 to 2.03731, saving model to model.h5\n",
      "Epoch 28/32\n",
      "9000/9000 [==============================] - 64s 7ms/step - loss: 0.5086 - val_loss: 2.0324\n",
      "\n",
      "Epoch 00028: val_loss improved from 2.03731 to 2.03241, saving model to model.h5\n",
      "Epoch 29/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.4653 - val_loss: 2.0372\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.03241\n",
      "Epoch 30/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.4251 - val_loss: 2.0437\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.03241\n",
      "Epoch 31/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.3914 - val_loss: 2.0413\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.03241\n",
      "Epoch 32/32\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.3595 - val_loss: 2.0603\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.03241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd2e415128>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=32, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[such weiter], target=[keep searching], predicted=[keep searching]\n",
      "src=[stoppen sie sie], target=[stop them], predicted=[stop them]\n",
      "src=[tom achzte], target=[tom groaned], predicted=[tom groaned]\n",
      "src=[ich habe kaviar gegessen], target=[i ate caviar], predicted=[i ate caviar]\n",
      "src=[ich habe tom vermisst], target=[ive missed tom], predicted=[i saw tom]\n",
      "src=[tom ist einfaltig], target=[tom is simple], predicted=[tom is simple]\n",
      "src=[ich schame mich so], target=[im so ashamed], predicted=[im so ashamed]\n",
      "src=[wie langweilig], target=[how boring], predicted=[how boring]\n",
      "src=[ich komme aus kanada], target=[im from canada], predicted=[im from canada]\n",
      "src=[wir verschwinden], target=[were leaving], predicted=[were leaving]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sudharshan\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Sudharshan\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Sudharshan\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.077869\n",
      "BLEU-2: 0.000000\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n",
      "test\n",
      "src=[das ist nicht schlimm], target=[thats not bad], predicted=[its not bad]\n",
      "src=[habe ich das gemacht], target=[did i do that], predicted=[did it say it]\n",
      "src=[denke daruber nach], target=[think about it], predicted=[now look]\n",
      "src=[weiter so tom], target=[way to go tom], predicted=[keep that for]\n",
      "src=[ich bin toms rivale], target=[im toms rival], predicted=[im toms]\n",
      "src=[tom sieht gut aus], target=[tom looks nice], predicted=[tom looks ok]\n",
      "src=[das leben ist schon seltsam], target=[life is strange], predicted=[life is]\n",
      "src=[hast du mich angerufen], target=[did you call me], predicted=[did you see]\n",
      "src=[tom schwanzt die schule], target=[tom cut school], predicted=[tom will well]\n",
      "src=[sagtet ihr dreiig], target=[did you say], predicted=[do something thirty]\n",
      "BLEU-1: 0.075240\n",
      "BLEU-2: 0.000000\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n",
    "model = load_model('model.h5')\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
